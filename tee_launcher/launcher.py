from collections import deque
import logging
import os
from typing import Dict, Union
import requests
from subprocess import CompletedProcess, run, check_output
import sys
import time
import traceback
from dataclasses import dataclass
import re
import ipaddress
import json


from requests.models import Response

MPC_CONTAINER_NAME = "mpc-node"

# The volume where this file resides is shared between launcher and app.
# To avoid concurrent modifications, the launcher mounts the volume read-only!
# the contents of this file are generated by the node itself and fetched from the contract.
IMAGE_DIGEST_FILE = "/mnt/shared/image-digest.bin"

# only considered if `IMAGE_DIGEST_FILE` does not exist.
ENV_VAR_DEFAULT_IMAGE_DIGEST = "DEFAULT_IMAGE_DIGEST"
# the time to wait between rpc requests, in milliseconds. Defaults to 500 milliseconds.
OS_ENV_VAR_RPC_REQUEST_INTERVAL_MS = "RPC_REQUST_INTERVAL_MS"
# the maximum time to wait for an rpc response. Defaults to 10 seconds.
OS_ENV_VAR_RPC_REQUST_TIMEOUT_SECS = "RPC_REQUST_TIMEOUT_SECS"
# the maximum number of attempts for rpc requests until we raise an exception
OS_ENV_VAR_RPC_MAX_ATTEMPTS = "RPC_MAX_RETRIES"
# MUST be set to 1.
OS_ENV_DOCKER_CONTENT_TRUST = "DOCKER_CONTENT_TRUST"

# optional - if set, overrides the approved hashes list.
# format: sha256:...
ENV_VAR_MPC_HASH_OVERRIDE = "MPC_HASH_OVERRIDE"


# dstack user configuration flags
DSTACK_USER_CONFIG_FILE = "/tapp/user_config"

# dstack user config. Read from `DSTACK_USER_CONFIG_FILE`
DSTACK_USER_CONFIG_MPC_IMAGE_TAGS = "MPC_IMAGE_TAGS"
DSTACK_USER_CONFIG_MPC_IMAGE_NAME = "MPC_IMAGE_NAME"
DSTACK_USER_CONFIG_MPC_IMAGE_REGISTRY = "MPC_REGISTRY"

# Default values for dstack user config file.
DEFAULT_MPC_IMAGE_NAME = "nearone/mpc-node-gcp"
DEFAULT_MPC_REGISTRY = "registry.hub.docker.com"
DEFAULT_MPC_IMAGE_TAG = "latest"

# the unix socket to communicate with dstack
DSTACK_UNIX_SOCKET = "/var/run/dstack.sock"

SHA256_PREFIX = "sha256:"


# Example of .user-config file format:
#
# MPC_ACCOUNT_ID=mpc-user-123
# MPC_LOCAL_ADDRESS=127.0.0.1
# MPC_SECRET_STORE_KEY=secret
# MPC_CONTRACT_ID=mpc-contract
# MPC_ENV=testnet
# MPC_HOME_DIR=/data
# NEAR_BOOT_NODES=boot1,boot2
# RUST_BACKTRACE=1
# RUST_LOG=info
# MPC_RESPONDER_ID=responder-xyz
# EXTRA_HOSTS=host1:192.168.0.1,host2:192.168.0.2
# PORTS=11780:11780,2200:2200

# Define an allow-list of permitted environment variables that will be passed to MPC container.
# Note - extra hosts and port forwarding are explicitly defined in the docker run command generation.
ALLOWED_MPC_ENV_VARS = {
    "MPC_ACCOUNT_ID",  # ID of the MPC account on the network
    "MPC_LOCAL_ADDRESS",  # Local IP address or hostname used by the MPC node
    "MPC_SECRET_STORE_KEY",  # Key used to encrypt/decrypt secrets
    "MPC_CONTRACT_ID",  # Contract ID associated with the MPC node
    "MPC_ENV",  # Environment (e.g., 'testnet', 'mainnet')
    "MPC_HOME_DIR",  # Home directory for the MPC node
    "NEAR_BOOT_NODES",  # Comma-separated list of boot nodes
    "RUST_BACKTRACE",  # Enables backtraces for Rust errors
    "RUST_LOG",  # Logging level for Rust code
    "MPC_RESPONDER_ID",  # Unique responder ID for MPC communication
    "MPC_BACKUP_ENCRYPTION_KEY_HEX",  # encryption key for backups
    # Workaround to allow MPC nodes to run against testnet
    # since nearcore 2.9 was never deployed there.
    # Set this to "now" to allow MPC nodes to sync with testnet 2.8.
    # See #[1379](https://github.com/near/mpc/issues/1379) for more context.
    "NEAR_TESTS_PROTOCOL_UPGRADE_OVERRIDE",
}

# Regex: hostnames must be alphanum + dash/dot, IPs must be valid IPv4
HOST_ENTRY_RE = re.compile(r"^[a-zA-Z0-9\-\.]+:\d{1,3}(\.\d{1,3}){3}$")
PORT_MAPPING_RE = re.compile(r"^(\d{1,5}):(\d{1,5})$")

# Updated regex to block any entry starting with '-' (including '--') and other unsafe characters
INVALID_HOST_ENTRY_PATTERN = re.compile(r"^[;&|`$\\<>-]|^--")


def is_safe_env_value(value: str) -> bool:
    """
    Ensures that an environment variable value does not contain dangerous substrings
    like LD_PRELOAD which may be used for injection.
    """
    if not isinstance(value, str):
        return False
    return "LD_PRELOAD" not in value


def is_valid_ip(ip: str) -> bool:
    try:
        ipaddress.ip_address(ip)
        return True
    except ValueError:
        return False


def is_valid_host_entry(entry: str) -> bool:
    if not HOST_ENTRY_RE.match(entry):
        return False
    host, ip = entry.split(":")
    return is_valid_ip(ip)


def is_valid_port_mapping(entry: str) -> bool:
    match = PORT_MAPPING_RE.match(entry)
    if not match:
        return False
    host_port, container_port = map(int, match.groups())
    return 0 < host_port <= 65535 and 0 < container_port <= 65535


def is_non_empty_and_cleaned(val: str) -> bool:
    if not isinstance(val, str):
        return False
    if not val.strip():
        return False
    return val.strip() == val


def is_safe_host_entry(entry: str) -> bool:
    """
    Ensure that host entry does not contain unsafe characters,
    does not start with '--' or '-', and does not include LD_PRELOAD.
    """
    if INVALID_HOST_ENTRY_PATTERN.search(entry):
        return False
    if "LD_PRELOAD" in entry:
        return False
    return True


def is_safe_port_mapping(mapping: str) -> bool:
    """Ensure that the port mapping does not contain unsafe characters or start with '--' or '-'."""
    return not INVALID_HOST_ENTRY_PATTERN.search(mapping)


def remove_existing_container():
    """Stop and remove the MPC container if it exists."""
    try:
        containers = check_output(
            ["docker", "ps", "-a", "--format", "{{.Names}}"], text=True
        ).splitlines()
        if MPC_CONTAINER_NAME in containers:
            logging.info(f"Removing existing container: {MPC_CONTAINER_NAME}")
            run(["docker", "rm", "-f", MPC_CONTAINER_NAME], check=False)
    except Exception as e:
        logging.warning(f"Failed to check/remove container {MPC_CONTAINER_NAME}: {e}")


@dataclass(frozen=True)
class ImageSpec:
    tags: list[str]
    image_name: str
    registry: str

    def __post_init__(self):
        if not self.tags or not all(is_non_empty_and_cleaned(tag) for tag in self.tags):
            raise ValueError(
                "tags must be a non-empty list of non-empty strings without whitespaces."
            )

        if not is_non_empty_and_cleaned(self.image_name):
            raise ValueError(
                "image_name must be a non-empty string without whitespaces."
            )

        if not is_non_empty_and_cleaned(self.registry):
            raise ValueError("registry must be a non-empty string without whitespaces.")


@dataclass(frozen=True)
class ResolvedImage:
    spec: ImageSpec
    digest: str

    def __post_init__(self):
        if not is_non_empty_and_cleaned(self.digest):
            raise ValueError(
                "image digest must be a non-empty string without whitespaces"
            )
            # should we require specific lengths?

    def name(self) -> str:
        return self.spec.image_name

    def tags(self) -> list[str]:
        return self.spec.tags

    def registry(self) -> str:
        return self.spec.registry


def parse_env_lines(lines: list[str]) -> dict:
    env = {}
    for line in lines:
        line = line.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        value = value.strip()
        if not key:
            continue
        env[key] = value
    return env


# Parses a .env-style file into a dictionary of key-value pairs.
def parse_env_file(path: str) -> dict:
    with open(path, "r") as f:
        return parse_env_lines(f.readlines())


def get_image_spec(dstack_config: dict[str, str]) -> ImageSpec:
    tags_values: list[str] = dstack_config.get(
        DSTACK_USER_CONFIG_MPC_IMAGE_TAGS, DEFAULT_MPC_IMAGE_TAG
    ).split(",")
    tags = [tag.strip() for tag in tags_values if tag.strip()]
    logging.info(f"Using tags {tags} to find matching MPC node docker image.")

    image_name: str = dstack_config.get(
        DSTACK_USER_CONFIG_MPC_IMAGE_NAME, DEFAULT_MPC_IMAGE_NAME
    )
    logging.info(f"Using image name {image_name}.")

    registry: str = dstack_config.get(
        DSTACK_USER_CONFIG_MPC_IMAGE_REGISTRY, DEFAULT_MPC_REGISTRY
    )
    logging.info(f"Using registry {registry}.")

    return ImageSpec(tags=tags, image_name=image_name, registry=registry)


def load_approved_hashes(dstack_config: dict) -> list[str]:
    """
    Load approved MPC image hashes from JSON file.

    Contract + MPC Node write them oldest → newest:
        ["sha256:h1", "sha256:h2", "sha256:h3"]

    Launcher must try newest → oldest:
        ["sha256:h3", "sha256:h2", "sha256:h1"]

    This function reverses the order to enforce this.
    """

    # No file → fallback to DEFAULT_IMAGE_DIGEST (single-entry list)
    if not os.path.isfile(IMAGE_DIGEST_FILE):
        fallback = os.environ[ENV_VAR_DEFAULT_IMAGE_DIGEST].strip()
        if not fallback.startswith(SHA256_PREFIX):
            fallback = SHA256_PREFIX + fallback
        logging.warning(
            f"{IMAGE_DIGEST_FILE} missing → fallback to DEFAULT_IMAGE_DIGEST={fallback}"
        )
        return [fallback]

    # JSON strictly required
    try:
        data = json.load(open(IMAGE_DIGEST_FILE))
    except Exception as e:
        raise RuntimeError(f"Failed to parse {IMAGE_DIGEST_FILE}: {e}")

    hashes = data.get("approved_hashes")
    if not isinstance(hashes, list) or not hashes:
        raise RuntimeError(
            f"Invalid JSON in {IMAGE_DIGEST_FILE}: approved_hashes missing or empty"
        )

    # Contract provides oldest→newest; launcher needs newest→oldest
    reversed_hashes = list(reversed(hashes))

    # Optional override: e.g. MPC_HASH_OVERRIDE=sha256:xyz
    override = dstack_config.get(ENV_VAR_MPC_HASH_OVERRIDE)
    if override and not override.startswith(SHA256_PREFIX):
        logging.warning(f"Ignoring invalid override format: {override}")
        override = None
    if override and override in reversed_hashes:
        reversed_hashes.remove(override)
        reversed_hashes.insert(0, override)
        logging.info(f"Startup order with override: {reversed_hashes}")
    else:
        logging.info(f"Startup order (newest→oldest): {reversed_hashes}")

    return reversed_hashes


def validate_image_hash(
    image_digest: str,
    dstack_config: dict,
    rpc_request_interval_secs: float,
    rpc_max_attempts: int,
) -> bool:
    """
    Returns True if the given image digest is valid (pull + manifest + digest match).
    Does NOT extend RTMR3 and does NOT run the container.
    """
    try:
        logging.info(f"Validating MPC hash: {image_digest}")

        image_spec = get_image_spec(dstack_config)
        docker_image = ResolvedImage(spec=image_spec, digest=image_digest)

        manifest_digest = get_manifest_digest(
            docker_image, rpc_request_interval_secs, rpc_max_attempts
        )

        name_and_digest = f"{image_spec.image_name}@{manifest_digest}"

        # Pull
        proc = run(["docker", "pull", name_and_digest], capture_output=True)
        if proc.returncode != 0:
            logging.error(f"docker pull failed for {image_digest}")
            return False

        # Verify digest
        proc = run(
            [
                "docker",
                "image",
                "inspect",
                "--format",
                "{{index .ID}}",
                name_and_digest,
            ],
            capture_output=True,
        )
        if proc.returncode != 0:
            logging.error(f"docker inspect failed for {image_digest}")
            return False

        pulled_digest = proc.stdout.decode("utf-8").strip()
        if pulled_digest != image_digest:
            logging.error(f"digest mismatch: {pulled_digest} != {image_digest}")
            return False

        return True

    except Exception as e:
        logging.error(f"Validation failed for {image_digest}: {e}")
        return False


def select_valid_hash(
    approved_hashes: list[str],
    dstack_config: dict,
    rpc_request_interval_secs: float,
    rpc_max_attempts: int,
) -> str:
    """
    Iterate through approved hashes, return the first hash that validates.
    Raises if none succeed.
    """
    for h in approved_hashes:
        if validate_image_hash(
            h, dstack_config, rpc_request_interval_secs, rpc_max_attempts
        ):
            logging.info(f"Valid MPC hash selected: {h}")
            return h

    raise RuntimeError("All approved MPC image hashes failed validation.")


def extend_rtmr3_and_launch(valid_hash: str, user_env: dict):
    """
    Extends RTMR3 with the selected valid hash and launches the MPC container.
    """
    logging.info(f"Extending RTMR3 with validated hash: {valid_hash}")

    proc = curl_unix_socket_post(
        endpoint="GetQuote", payload='{"report_data": ""}', capture_output=True
    )
    if proc.returncode:
        raise RuntimeError("GetQuote failed before extending RTMR3")

    extend_rtmr3_json = (
        '{"event": "mpc-image-digest", "payload": "%s"}' % valid_hash.split(":")[1]
    )

    proc = curl_unix_socket_post(
        endpoint="EmitEvent", payload=extend_rtmr3_json, capture_output=True
    )
    if proc.returncode:
        raise RuntimeError("EmitEvent failed while extending RTMR3")

    # Launch container
    logging.info(f"Launching MPC node with validated hash: {valid_hash}")

    remove_existing_container()
    docker_cmd = build_docker_cmd(user_env, valid_hash)

    proc = run(docker_cmd)
    if proc.returncode != 0:
        raise RuntimeError(f"docker run failed for validated hash={valid_hash}")

    logging.info("MPC launched successfully.")


def curl_unix_socket_post(
    endpoint: str, payload: Union[str, bytes], capture_output: bool = False
) -> CompletedProcess:
    """
    Send a POST request via curl using the DSTACK UNIX socket.

    Python's requests package cannot natively talk HTTP over a unix socket (which is the API
    exposed by dstack's guest agent). To avoid installing another Python depdendency, namely
    requests-unixsocket, we just use curl.

    Args:
        endpoint: Path after `http://dstack/`, e.g. 'GetQuote', 'EmitEvent'
        payload: JSON string or bytes to send as the request body
        capture_output: Whether to capture stdout/stderr (default: False)

    Returns:
        subprocess.CompletedProcess result
    """
    url = f"http://dstack/{endpoint}"
    cmd = [
        "curl",
        "--unix-socket",
        DSTACK_UNIX_SOCKET,
        "-X",
        "POST",
        url,
        "-H",
        "Content-Type: application/json",
        "-d",
        payload,
    ]
    return run(cmd, capture_output=capture_output)


def main():
    logging.info("start")

    # DOCKER_CONTENT_TRUST must be enabled
    if os.environ.get(OS_ENV_DOCKER_CONTENT_TRUST, "0") != "1":
        raise RuntimeError(
            "Environment variable DOCKER_CONTENT_TRUST must be set to 1."
        )

    # Load dstack configuration (tags, registry, image name)
    # In dstack, /tapp/user_config provides unmeasured data to the CVM.
    # We use this interface to make some aspects of the launcher configurable.
    # *** Only security-irrelevant parts *** may be made configurable in this way, e.g., the specific image tag(s) we look up.
    dstack_config: dict[str, str] = (
        parse_env_file(DSTACK_USER_CONFIG_FILE)
        if os.path.isfile(DSTACK_USER_CONFIG_FILE)
        else {}
    )

    # RPC timing configuration
    int(os.environ.get(OS_ENV_VAR_RPC_REQUST_TIMEOUT_SECS, "10"))  # TODO used?
    rpc_request_interval_ms = int(
        os.environ.get(OS_ENV_VAR_RPC_REQUEST_INTERVAL_MS, "1000")
    )
    rpc_request_interval_secs = rpc_request_interval_ms / 1000.0
    rpc_max_attempts = int(os.environ.get(OS_ENV_VAR_RPC_MAX_ATTEMPTS, "20"))

    # --------------------------------------------------------
    # Load approved hashes from disk.
    # --------------------------------------------------------
    approved_hashes = load_approved_hashes(dstack_config)

    logging.info(f"Approved hashes: {approved_hashes}")

    # --------------------------------------------------------
    # PHASE 1: VALIDATION — find a single valid hash
    # --------------------------------------------------------
    valid_hash = select_valid_hash(
        approved_hashes,
        dstack_config,
        rpc_request_interval_secs,
        rpc_max_attempts,
    )

    # --------------------------------------------------------
    # PHASE 2 + 3:
    # - Extend RTMR3 with validated hash
    # - Launch MPC container
    # --------------------------------------------------------
    extend_rtmr3_and_launch(valid_hash, dstack_config)


def request_until_success(
    url: str,
    headers: Dict[str, str],
    rpc_request_interval_secs: float,
    rpc_request_timeout_secs: float,
    rpc_max_attempts: int,
) -> Response:
    """
    Repeatedly sends a GET request to the specified URL until a successful (200 OK) response is received.

    Args:
        url (str): The URL to request.
        headers (Dict[str, str]): Optional headers to include in the request.
        rpc_request_interval_secs (float): Time in seconds to wait between retries on failure.
        rpc_request_timeout_secs (float): Maximum time in seconds to wait for a request to succeed.

    Returns:
        Response: The successful HTTP response object with status code 200.

    Notes:
        - Retries indefinitely until the request succeeds.
        - Prints a warning with the response content on each failure.
    """
    for attempt in range(1, rpc_max_attempts + 1):
        # Ensure that we respect the backoff time. Performance is not a priority in this case.
        time.sleep(rpc_request_interval_secs)
        rpc_request_interval_secs = min(max(rpc_request_interval_secs, 1.0) * 1.5, 60.0)
        try:
            manifest_resp = requests.get(
                url, headers=headers, timeout=rpc_request_timeout_secs
            )
        except requests.exceptions.Timeout:
            print(
                f"[Warning] Attempt {attempt}/{rpc_max_attempts}: Failed to fetch {url} for headers {headers}. "
                f"Status: Timeout"
            )
            continue
        if manifest_resp.status_code != 200:
            print(
                f"[Warning] Attempt {attempt}/{rpc_max_attempts}: Failed to fetch {url} for headers {headers}. "
                f"Status: {manifest_resp.text} {manifest_resp.headers}"
            )
            continue
        else:
            return manifest_resp

    raise RuntimeError(
        f"Failed to get succesful response from {url} after {rpc_max_attempts} attempts."
    )


def get_manifest_digest(
    docker_image: ResolvedImage, rpc_request_interval_secs: float, rpc_max_attempts: int
) -> str:
    """
    Given an `image_digest` returns a manifest digest.

       `docker pull` requires a manifest digest. This function translates an image digest into a manifest digest by talking to the Docker registry.

       API doc for image registry https://distribution.github.io/distribution/spec/api/
    """
    if not docker_image.tags():
        raise Exception(f"No tags found for image {docker_image.spec.image_name}")

    # We need an authorization token to fetch manifests.
    # TODO this still has the registry hard-coded in the url. also, if we use a different registry, we need a different auth-endpoint.
    token_resp = requests.get(
        f"https://auth.docker.io/token?service=registry.docker.io&scope=repository:{docker_image.name()}:pull"
    )
    token_resp.raise_for_status()
    token = token_resp.json().get("token", [])

    tags = deque(docker_image.tags())

    while tags:
        tag = tags.popleft()

        manifest_url = f"https://{docker_image.registry()}/v2/{docker_image.name()}/manifests/{tag}"
        headers = {
            "Accept": "application/vnd.docker.distribution.manifest.v2+json",
            "Authorization": f"Bearer {token}",
        }
        try:
            manifest_resp = request_until_success(
                url=manifest_url,
                headers=headers,
                rpc_request_interval_secs=rpc_request_interval_secs,
                rpc_request_timeout_secs=rpc_request_interval_secs,
                rpc_max_attempts=rpc_max_attempts,
            )
            manifest = manifest_resp.json()
            match manifest["mediaType"]:
                case "application/vnd.oci.image.index.v1+json":
                    # Multi-platform manifest; we scan for amd64/linux images and add them to `tags`
                    for image_manifest in manifest.get("manifests", []):
                        platform = image_manifest.get("platform", [])
                        if (
                            platform.get("architecture") == "amd64"
                            and platform.get("os") == "linux"
                        ):
                            tags.append(image_manifest["digest"])
                case (
                    "application/vnd.docker.distribution.manifest.v2+json"
                    | "application/vnd.oci.image.manifest.v1+json"
                ):
                    config_digest = manifest["config"]["digest"]
                    if config_digest == docker_image.digest:
                        return manifest_resp.headers["Docker-Content-Digest"]
        except RuntimeError as e:
            print(
                f"[Warning] {e}: Exceeded number of maximum RPC requests for any given attempt. Will continue in the hopes of finding the matching image hash among remaining tags"
            )
            # Q: Do we expect all requests to succeed?

    raise Exception("Image hash not found among tags.")


def build_docker_cmd(user_env: dict[str, str], image_digest: str) -> list[str]:
    # Parse the image hash safely
    if ":" in image_digest:
        parts = image_digest.split(":", 1)
        if len(parts) == 2 and parts[1]:
            image_hash = parts[1]
        else:
            raise ValueError(f"Invalid image_digest format: {image_digest}")
    else:
        image_hash = image_digest

    docker_cmd = ["docker", "run"]

    # add required environment variables.
    # "MPC_IMAGE_HASH",  -  Digest of the Docker image - used by the MPC node to verify hash used.
    # "MPC_LATEST_ALLOWED_HASH_FILE" - Path to the shared digest file
    docker_cmd += ["--env", f"MPC_IMAGE_HASH={image_hash}"]
    docker_cmd += ["--env", f"MPC_LATEST_ALLOWED_HASH_FILE={IMAGE_DIGEST_FILE}"]

    # Set the MPC configuration to run in real TDX mode.
    docker_cmd += ["--env", f"DSTACK_ENDPOINT={DSTACK_UNIX_SOCKET}"]

    for key, value in user_env.items():
        if key in ALLOWED_MPC_ENV_VARS:
            if is_safe_env_value(value):
                docker_cmd += ["--env", f"{key}={value}"]
            else:
                logging.warning(
                    f"Ignoring environment variable with unsafe value: {key}"
                )
        elif key == "EXTRA_HOSTS":
            for host_entry in value.split(","):
                clean_host = host_entry.strip()
                if is_safe_host_entry(clean_host) and is_valid_host_entry(clean_host):
                    docker_cmd += ["--add-host", clean_host]
                else:
                    logging.warning(
                        f"Ignoring invalid or unsafe EXTRA_HOSTS entry: {clean_host}"
                    )
        elif key == "PORTS":
            for port_pair in value.split(","):
                clean_port = port_pair.strip()
                if is_safe_port_mapping(clean_port) and is_valid_port_mapping(
                    clean_port
                ):
                    docker_cmd += ["-p", clean_port]
                else:
                    logging.warning(
                        f"Ignoring invalid or unsafe PORTS entry: {clean_port}"
                    )
        else:
            logging.info(f"Ignoring non-MPC variable: {key}")

    docker_cmd += [
        "--security-opt",
        "no-new-privileges:true",
        "-v",
        "/tapp:/tapp:ro",
        "-v",
        f"{DSTACK_UNIX_SOCKET}:{DSTACK_UNIX_SOCKET}",
        "-v",
        "shared-volume:/mnt/shared",
        "-v",
        "mpc-data:/data",
        "--name",
        MPC_CONTAINER_NAME,
        "--detach",
        image_digest,
    ]
    logging.info("docker cmd %s", " ".join(docker_cmd))

    # Final safeguard: ensure LD_PRELOAD isn't anywhere in the command
    if any("LD_PRELOAD" in arg for arg in docker_cmd):
        raise RuntimeError(
            "Unsafe docker command: LD_PRELOAD detected in argument list."
        )

    # Also check the full command as a single string
    docker_cmd_str = " ".join(docker_cmd)
    if "LD_PRELOAD" in docker_cmd_str:
        raise RuntimeError("Unsafe docker command string: LD_PRELOAD detected.")

    return docker_cmd


if __name__ == "__main__":
    try:
        logging.basicConfig(
            level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
        )

        main()
        sys.exit(0)
    except Exception as e:
        print("Error:", str(e), file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.exit(1)
