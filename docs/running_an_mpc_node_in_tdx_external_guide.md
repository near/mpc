# MPC Node Deployment in TDX  \- External Guide

# Introduction

Chain signatures is a Multi-Party Computation (MPC) service that lets you sign arbitrary payloads by calling a smart contract and receiving a signature. The returned signature can be used to derive public keys on external chains (for example, Ethereum or Bitcoin).

This guide walks you through deploying a self-hosted MPC node on a bare-metal server using Intel Trusted Domain Extensions (TDX). The node runs inside a Confidential VM (CVM) for isolation and attestation.

We use Dstack (from Phala) to orchestrate the environment and run the MPC container inside the CVM

# Limitations and Restrictions

 **Important:**
You cannot migrate an existing MPC node out of its CVM without data loss (for example: key share, P2P key). In addition, replacing or changing TDX-related hardware or dependencies (e.g., a CPU swap) may render the data unrecoverable.

# Main difference between TEE and non TEE MPC nodes.

From an operator’s perspective, the key differences between a **TEE-based** MPC node and a **non-TEE** node are:

For a full architecture review of the TEE-based MPC, see: [design doc](securing_mpc_with_tee_design_doc.md)



| Category/feature | Non TEE | With TEE |
| :---- | :---- | :---- |
|  |  |  |
| Hardware Setup | Any hardware that meets the bandwidth,compute and memory requirements.  | Additional requirements to support Intel TDX Hardware.  |
| Deployment  | Deploy an MPC Docker image directly. | Use Dstack interface to deploy a Launcher docker image that will deploy the MPC node.  |
| Node Account key and P2P key | Generated by the operator, and provided to the node. | Generated by the node. Private keys are never exposed outside of  the CVM. Public keys are exported by  the node. And added to the operator's account. |
| Debug, Recovery| Operator has complete control over the environment | Operator has no access into the CVM (except for defined entry points) |  



# Prerequisites and Requirements

## Hardware Requirements 

* Have a TDX enabled, bare metal, server.  
    
  Note \- we currently only support bare metal and  do not support virtualized TDX solutions (such as GCP)  
  

* Intel Xeon 5th/6th Generation CPU (TDX Support) and 8 RAM slots filled  
  See [Intel TDX HW requirements](https://cc-enabling.trustedservices.intel.com/intel-tdx-enabling-guide/03/hardware_selection/)  
* Memory \- 64GB  
* (v)Cores \- 8  
* Disk space \- 500GB, SSD NVMe or similar performance

For a list of supported cloud providers offering bare metal servers with Intel TDX, see [Cloud Providers Supporting Bare Metal Servers with Intel TDX](./cloud-providers-tdx.md).


##  General 

* Firewall:allow ingress port 80 (MPC), 24567 (near) and port 8080 (web)  
* Assign a static public IP for access towards machine from outside

## Create DNS A record (optional). 

Although a node can be accessed using a public IP address, it is recommended to use a domain name instead. Using a domain name allows some flexibility in case of public IP address change/repurpose or failover scenarios. To use a domain name, one must register a DNS A record. Some recommended providers:

* [Namecheap](https://www.namecheap.com/support/knowledgebase/article.aspx/319/2237/how-can-i-set-up-an-a-address-record-for-my-domain/)  
* [Cloudflare](https://developers.cloudflare.com/dns/manage-dns-records/how-to/create-dns-records/)

## TDX and Dstack setup

This section describes how to enable TDX on your machine (BIOS, Operating System (OS) and software configurations), and also describes how to install and configure Dstack. 

Make sure to follow the 3 steps below in order to have a working TDX machine with Dstack setup.

1. A bare metal TDX server setup following [canonical/tdx](https://github.com/canonical/tdx/blob/main/README.md)  
   Follow steps 1-4 (basic TDX setup configuration)  and steps 9.1-9.2 (Setup Remote Attestation) in the canonical guide to make sure you have a working TDX setup.

2. Dstack setup following [https://github.com/Dstack-TEE/dstack](https://github.com/Dstack-TEE/dstack)  
   Follow the steps in [build-and-play-locally](https://github.com/Dstack-TEE/dstack?tab=readme-ov-file#build-and-play-locally) up until (but not including) “[deploy an App](https://github.com/Dstack-TEE/dstack?tab=readme-ov-file#deploy-an-app)”.  
   Note \- you won’t need to enable or run KMS and TProxy (gateway).  
   


[#693](https://github.com/near/mpc/issues/693) 
[#635](https://github.com/near/mpc/issues/635) 

TBD  [#894](https://github.com/near/mpc/issues/894)  : We need to choose a specific github commit and use the reproducible build script  in order to  generate the OS image. [https://github.com/Dstack-TEE/meta-dstack?tab=readme-ov-file\#reproducible-build-the-guest-image](https://github.com/Dstack-TEE/meta-dstack?tab=readme-ov-file#reproducible-build-the-guest-image)  
Or we can choose a specific dstack image like 0.52

3. Setup local gramine-sealing-key-provider (see details [below](#setting-up-local-gramine-sealing-key-provider:)).  
   

### Dstack Configuration
In this section we describe specific Dstack configuration changes needed for deploying the MPC node in Dstack.

In vmm.toml: 

- Update `cvm.port_mapping.range` to include port number **24567**  
- In the `[cvm]` section add: `max_disk_size = 1000`


### Setting up a Local Gramine-Sealing-Key-Provider 


In this solution, we use the `gramine-sealing-key-provider`, which runs inside an SGX enclave, to generate a key.  
This key is derived from TDX measurements and the SGX enclave’s hardware sealing key, and it is used to encrypt the CVM’s file system.  

> **Note:** This key is tied to the platform. Losing it will prevent the CVM from decrypting the drive on subsequent VM boots.

For more information, see [local-key-provider-from-phala](https://github.com/Dstack-TEE/dstack/tree/master/kms#local-key-provider-mode-1).  

#### Setup Instructions

1. Follow the [canonical/tdx setup](#) if not already completed — especially step 9.1–2 (establishing an SGX PCCS: Provisioning Certification Caching Service).

2. Deploy an instance of `gramine-sealing-key-provider` on the host machine.  
   - On the DTX server, run the script [run.sh](https://github.com/Dstack-TEE/dstack/blob/master/key-provider-build/run.sh)  
   
   > **Prerequisite:** Docker must be installed.

3. To find the `mr_enclave` value of the SGX key provider, run:
   ```bash
   docker logs gramine-sealing-key-provider 2>&1 | grep mr_enclave | head -n 1
   ```

   Ensure that the `mr_enclave` matches the expected value:
   ```
   1b7a49378403249b6986a907844cab0921eca32dd47e657f3c10311ccaeccf8b
   ```
 
 **Note**: As part of the mutual attestation between the CVM and the key provider, the CVM will check that the key provider’s `mr_enclave` matches the above hash.


# MPC Node Setup and Deployment

This section will describe how to configure and deploy your MPC node inside a CVM.

Including 

*  Creating a Near account for your node

*  Preparing  a configuration file based on [user-config.conf](https://github.com/near/mpc/blob/main/tee_launcher/user-config.conf)

* Creating a docker compose file for the launcher based on [launcher\_docker\_compose.yaml](https://github.com/near/mpc/blob/main/tee_launcher/launcher_docker_compose.yaml).  
* Configuring and starting your CVM with the MPC node.  
* Accessing mpc docker logs.  
* Retrieve keys from the CVM.  
* Add the node key to your Near account.
## Create a NEAR Account for Your Node


> **Important** – In the following examples, the account keys are auto-generated as part of the command. But it is also possible to create the keys separately and add them to the account creation command.  
>  
> In either case, it is the operator's full responsibility to manage and protect these keys.  
>  
> See the [NEAR CLI](https://github.com/near/near-cli-rs/blob/main/docs/GUIDE.en.md) documentation for more options and details.

### **Mainnet**

A named mainnet account is required. To create one, you can use a known wallet like [https://www.mynearwallet.com](https://www.mynearwallet.com) or [https://wallet.meteorwallet.app](https://wallet.meteorwallet.app), or fund it yourself:

```
$ near account create-account fund-myself <ACCOUNT_NAME> '<AMOUNT TO FUND> NEAR' autogenerate-new-keypair save-to-keychain sign-as <SIGNER_ACCOUNT_ID> network-config mainnet
```

### **Testnet**

If you're using testnet, the easiest way to get started is to create an account sponsored by the faucet — the [NEAR command line interface](https://github.com/near/near-cli-rs) can set this up for you. Public and private keys are generated during this process. You can also use an existing account if you have one.


Using auto-generated keypair:
```
$ near account create-account sponsor-by-faucet-service <ACCOUNT_NAME> autogenerate-new-keypair save-to-keychain network-config testnet create
```

Using a manually provided public key:
```
$ near account create-account sponsor-by-faucet-service <ACCOUNT_NAME> use-manually-provided-public-key <PUBLIC_KEY> network-config testnet create
```

For more details, please refer to the NEAR account documentation.


##   Prepare MPC container environment variables

Create a user-config.conf file based on the [user-config.conf](https://github.com/near/mpc/blob/main/tee_launcher/user-config.conf) . 

```
# MPC docker image local override
MPC_IMAGE_NAME=nearone/mpc-node-gcp
MPC_IMAGE_TAGS=latest
MPC_REGISTRY=registry.hub.docker.com

# MPC node settings
MPC_ACCOUNT_ID=$MY_MPC_NEAR_ACCOUNT_ID
MPC_LOCAL_ADDRESS=127.0.0.1
MPC_SECRET_STORE_KEY=AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
MPC_CONTRACT_ID=$CONTRACT_ID # v1.signer-prod.testnet for Testnet or v1.signer for Mainnet
MPC_ENV=$CHAIN_ID # testnet or mainnet
MPC_HOME_DIR=/data
RUST_BACKTRACE=full
RUST_LOG=mpc=debug,info

NEAR_BOOT_NODES=$BOOT_NODES


# Port forwarding 
PORTS=8080:8080,24567:24567,80:80

```

Adjust the variables as per your environment.

* MPC_ACCOUNT_ID `-` use the near account ID that was created in the previous step  
  MPC\_CONTRACT\_ID is **v1.signer-prod.testnet** for testnet and **v1.signer** for mainnet 
* PORTS: Those are the port forwarding rules for the MPC container. Those should be a subset of the port forwarding for the CVM that are defined [Port Mapping](https://github.com/near/mpc/blob/main/docs/running_an_mpc_node_in_tdx_external_guide.md#using-the-web-interface)
*  A fresh set of boot nodes can be selected using Testnet/Mainnet RPC endpoints. Copy at least 4-5 nodes from curl results into NEAR\_NOOT\_NODES variable:

```
curl -X POST https://rpc.[testnet|mainnet].near.org \
  -H "Content-Type: application/json" \
  -d '{
        "jsonrpc": "2.0",
        "method": "network_info",
        "params": [],
        "id": "dontcare"
      }'| \
jq -r '.result.active_peers[]  as $active_peer  | "\($active_peer.id)@\($active_peer.addr)"' |\
paste -sd',' -
```



## Preparing a Docker Compose File

To launch the MPC node in the TEE environment, use the Docker Compose file from the [NEAR MPC repository](https://github.com/near/mpc/blob/main/tee_launcher/launcher_docker_compose.yaml).

Update the `DEFAULT_IMAGE_DIGEST` field in `launcher_docker_compose.yaml` with the latest MPC Docker image digest retrieved from the contract.

For details on how to map this hash to a specific Docker image published on DockerHub, or to the corresponding source code, see the section [MPC Node Upgrades](#mpc-node-upgrades).


Example digest value:
```bash
DEFAULT_IMAGE_DIGEST=sha256:4b08c2745a33aa28503e86e33547cc5a564abbb13ed73755937ded1429358c9d
```

You can retrieve the latest MPC Docker image hash directly from the contract using the NEAR CLI:

```bash
near contract call-function as-transaction \
  v1.signer-prod.testnet \
  latest_code_hash \
  json-args '{}' \
  prepaid-gas '100.0 Tgas' \
  attached-deposit '0 NEAR' \
  sign-as <your-account-id> \
  network-config testnet \
  sign-with-keychain \
  send
```

The transaction output will include the latest MPC Docker image hash.



**Note:** -  the [launcher\_docker\_compose.yaml](https://github.com/near/mpc/blob/main/tee_launcher/launcher_docker_compose.yaml) is measured, and the measurements are part of the remote attestation. Make sure not to change any other fields or values (including  whitespaces).


## Required Ports and Port Collisions 

MPC nodes use a fixed set of ports for communication and telemetry.  
This creates a limitation when trying to run both **mainnet** and **testnet** nodes on the same physical server, since both sets of nodes attempt to bind to the same ports.

---

- **Single network per machine**: By default, running both mainnet and testnet on the same machine is not supported because of port collisions.  
- **Workaround with multiple IPs**: It is possible to run multiple nodes (e.g., one mainnet and one testnet) on the same host if the server is configured with **multiple external IP addresses**.  
  - Each node binds to the required ports (see below) on a separate IP.  
  - Additional IP/port routing on the local machine may be required.  

---

### Required Ports

| Port   | Purpose                                                                 |
|--------|-------------------------------------------------------------------------|
| **80** | Node-to-node communication (port override convention)                   |
| **24567** | Decentralized state sync                                               |
| **8080** | Debug and telemetry collection, plus the new `getdata` endpoint         |
| **3030** | Debug and telemetry collection                                         |


## Configuring and starting the MPC binary in a CVM

There are 2 ways to manage the VM that will run the MPC node.

1\. Using the Web interface  
2\. Using a script.

Note \- both methods provide the same functionality. The Web interface provides a more manual approach and control. While the script is useful for automating processes.  

### **Using the Web interface** 

Follow the [Dstack guide](https://github.com/Dstack-TEE/dstack?tab=readme-ov-file#deploy-an-app) (deploy an App): 

Use the following custom settings for MPC:

1. Launcher docker compose file \- provided above.  
2. VM HW setting: (use exactly those settings, since vCPU/Memory are measured )  
    vCPU number=8  ,   Memory \= 64GB, disk \= 500 GB  
3. Pre script  \- empty.  
4. user-config \- provided above  
5. KMS=disable, Local Keyprovier=enabled, Tproxy=disable, public logs=enabled,public sysinfo=enabled,pin NUMA=disabled  
6. Port mapping: (taken from the list above)  
   Public 80:80  (main node to node communication port)   
   Public 24567:24567 (required for decentralized state sync)  
   Public 8080:8080  (required for collecting debug and telemetry information)   
   Local 3030:3030:   (use public with you want the debug metrics to be available on the internet)  
   Local <dstack_agent_port>:8090: (required for access CVM information and container logs)   
     
7.  Key Provider ID: (The MrEnclave for the sgx local key provider) 1b7a49378403249b6986a907844cab0921eca32dd47e657f3c10311ccaeccf8b

 

![CVM Web Page](./attachments/VMM_web_page.png)


###  Using the script: 

Use the script [deploy-launcher.sh](https://github.com/near/mpc/blob/main/tee_launcher/deploy-launcher.sh) described here   
[https://github.com/near/mpc/blob/main/tee\_deployment/deploy\_launcher\_guide.md](https://github.com/near/mpc/blob/main/tee_launcher/deploy_launcher_guide.md)  
To configure and start your VM.

## Accessing MPC (or Launcher) Docker Logs

### Overview
Dstack provides a dedicated web page to view CVM information, including links to the Docker logs.  
More details can be found in [Phala's guide](https://github.com/Dstack-TEE/dstack?tab=readme-ov-file#deploy-an-app).  

---

### Local Access
The web page is available on the **TDX server**  at **`dstack_agent_port`** configured earlier in [Using the Web Interface](#using-the-web-interface).  

Open in your browser:  

```
http://localhost:<dstack_agent_port>
```

---

### Remote Access
If you need to access the web page from another machine, set up SSH port forwarding.  

For example, if `dstack_agent_port = 8090`:  

```bash
ssh -NL 17190:localhost:8090 USER_NAME@TDX_SERVER
```

Then open:  

```
http://localhost:17190
```

---

### Example / Screenshot
![CVM Web Page](./attachments/CVM_web_page.png)


## Retrieve public keys from the MPC node. 

There are 2 keys that should be retrieved from node.  

* P2P  (near\_p2p\_public\_key)- this key is used by the nodes to authenticate with one another. This key needs to be registered on the contract. (see details below)  
* Node Account Key (near\_signer\_public\_key) \- this key is used by the node to issue operations such as “vote\_reshared”.  
  This key needs to be added to the Near account that was created in the step above.

## Retrieve the node account key and P2P key.

In order to retrieve the node account key and the P2P key. On your server   
Call the HTTP end point [http://localhost:8080/public\_data](http://localhost:8080/public_data)  \- and search for near\_signer\_public\_key and near\_p2p\_public\_key

```
{"near_signer_public_key":"ed25519:B2JvaYmgzfXsvCxrqd4nBrBt8jo9ReqUZatG3dAZEBv5","near_p2p_public_key":"ed25519:5SiS1SJiABiM79Yt6uEjMabAT9UguQY9hSyF7xfHLGYt"
```

Sample curl command to extract the keys:

```
$curl -s http://<IP>:8080/public_data | jq -r '.near_signer_public_key'
$ed25519:B2JvaYmgzfXsvCxrqd4nBrBt8jo9ReqUZatG3dAZEBv5$curl -s http://<IP>:8080/public_data | jq -r '.near_p2p_public_key'$ed25519:5SiS1SJiABiM79Yt6uEjMabAT9UguQY9hSyF7xfHLGYt

```
## Add the Node Account Key to Your Account

This section shows how to add the MPC node's public key (from the previous section) as a **restricted function-call access key** to your NEAR account using the previously mentioned NEAR-CLI, allowing the MPC node to interact with the **MPC signer contract**.

---

### Parameters

- **`ACCOUNT_ID`** → The NEAR account that will own the new key.  
  Example: `your-node-account.testnet`

- **`MPC_CONTRACT_ID`** → The MPC signer contract ID:  
  
  Testnet:   v1.signer-prod.testnet  
  Mainnet:   v1.signer
  

- **`MPC_NODE_PUBLIC_KEY`** → The public key of the MPC node you want to add.  
  Example: `ed25519:ABCDEFG...`

- **`METHOD_NAMES`** → The list of contract methods the MPC node is allowed to call:  
  ```
  respond,
  respond_ckd,
  vote_pk,
  start_keygen_instance,
  vote_reshared,
  start_reshare_instance,
  vote_abort_key_event_instance,
  verify_tee,
  submit_participant_info
  ```

---

### Example Command

```bash
./target/release/near account add-key $ACCOUNT_ID   grant-function-call-access   --allowance '1 NEAR'   --contract-account-id $MPC_CONTRACT_ID   --function-names $METHOD_NAMES   use-manually-provided-public-key $MPC_NODE_PUBLIC_KEY   network-config testnet   sign-with-keychain   send
```

---

### Sample Bash Script

```bash
#!/bin/bash

# === Configuration ===
ACCOUNT_ID="your-node-account.testnet"
MPC_CONTRACT_ID="v1.signer-prod.testnet"    # use "v1.signer" for mainnet
MPC_NODE_PUBLIC_KEY="ed25519:YOUR_PUBLIC_KEY_HERE"
ALLOWANCE="1 NEAR"
NETWORK="testnet"   # or "mainnet"

# Methods the MPC node is allowed to call
METHOD_NAMES="respond,respond_ckd,vote_pk,start_keygen_instance,vote_reshared,start_reshare_instance,vote_abort_key_event_instance,verify_tee,submit_participant_info"

# === Add Access Key ===
./target/release/near account add-key $ACCOUNT_ID   grant-function-call-access   --allowance "$ALLOWANCE"   --contract-account-id $MPC_CONTRACT_ID   --function-names $METHOD_NAMES   use-manually-provided-public-key $MPC_NODE_PUBLIC_KEY   network-config $NETWORK   sign-with-keychain   send
```

---

### Verification


After sending the transaction, check that the new key was added:

```bash
./target/release/near account list-keys $ACCOUNT_ID \
  network-config $NETWORK \
  now
```


# Joining the MPC Cluster

After the MPC node has been deployed and its NEAR account key successfully added to the operator's account, the node will attempt to sync and then submit its attestation information to the contract.

Once these steps are complete, the operator should request all other operators to vote for adding the new MPC node by calling the **vote_new_parameters** method.

## Wait for NEAR Indexer to Sync

Wait until the NEAR Indexer has completed state sync. This process can take several hours. You can check the progress in the Docker container logs or via the metrics endpoint:

```bash
curl http://127.0.0.1:3030/metrics | grep near_sync_status
# Sync is completed when status is 0: 
near_sync_status 0
```

## Submitting Participant Info

> **Note:** During the [transition phase](#transition-phase), this step is optional. The contract will accept nodes that do not submit an attestation.  

Once the MPC node is fully synced, it will call `submit_participant_info` to submit its attestation information to the contract.

If the node’s key has not been added to the account, this operation will fail. In that case, the node will retry the operation in a loop.

> **Note:** This behavior is not yet implemented. See issue [#1069](https://github.com/near/mpc/issues/1069).  

> **TBD [#1079]:** Add screenshot/logs/cURL example for detecting when the MPC node has submitted attestation information.

> **Note:** Calling this method will incur a cost (TBD, XXX NEAR). Ensure this amount is available in your account.  
> _(TBD [#903](https://github.com/near/mpc/issues/903) – confirm exact cost)_


## Voting: (vote_new_parameters)


Ask other members to vote for your candidacy via the `vote_new_parameters` contract method.

To generate a voting command, follow these steps:

1. **Get the current state**
   ```bash
   near contract call-function as-read-only v1.signer-prod.testnet state json-args {} network-config testnet now
   ```

   Example output (truncated for clarity):
   ```json
   {
     "Running": {
       "keyset": {
         "domains": [
           {
             "attempt": 0,
             "domain_id": 0,
             "key": {
               "Secp256k1": {
                 "near_public_key": "secp256k1:4NfTiv3UsGahebgTaHyD9vF8KYKMBnfd6kh94mK6xv8fGBiJB8TBtFMP5WWXz6B89Ac1fbpzPwAvoyQebemHFwx3"
               }
             }
           },
           {
             "attempt": 0,
             "domain_id": 1,
             "key": {
               "Ed25519": {
                 "near_public_key_compressed": "ed25519:6vSEtQxrQj6txUMh33WC4ERyCWmNMRTdufDWAaDY3Un2"
               }
             }
           }
         ],
         "epoch_id": 10
       },
       "parameters": {
         "participants": {
           "next_id": 12,
           "participants": [
             [
               "aurora-multichain.testnet",
               0,
               { "sign_pk": "ed25519:BSgizrs...", "url": "http://34.49.211.4" }
             ],
             [
               "bst-near.testnet",
               1,
               { "sign_pk": "ed25519:AadQTC...", "url": "http://34.98.94.79" }
             ]
             ...
             ...
             ...
           ]
         },
         "threshold": 5
     
       }
     }
   }
   ```

   From this output, extract:
   - `participants`
   - `epoch_id` (10 in this example)
   - `next_id` (12 in this example)

2. **Update the state**  
   - Add your new participant to the `participants` array.  
   - Set `prospective_epoch_id = epoch_id + 1` (11 in this example).  
   - Set `next_id = next_id + 1` (13 in this example).  

   ### Before
   ```json
   "participants": {
     "next_id": 12,
     "participants": [
       [
         "aurora-multichain.testnet",
         0,
         { "sign_pk": "ed25519:BSgizrs...", "url": "http://34.49.211.4" }
       ],
       [
         "bst-near.testnet",
         1,
         { "sign_pk": "ed25519:AadQTC...", "url": "http://34.98.94.79" }
       ]
       ...
       ...
       ...
     ]
   }
   ```

   ### After (new participant `new-node.testnet` added)
   ```json
   "participants": {
     "next_id": 13,
     "participants": [
       [
         "aurora-multichain.testnet",
         0,
         { "sign_pk": "ed25519:BSgizrs...", "url": "http://34.49.211.4" }
       ],
       [
         "bst-near.testnet",
         1,
         { "sign_pk": "ed25519:AadQTC...", "url": "http://34.98.94.79" }
       ],
       [
         "new-node.testnet",
         12,
         { "sign_pk": "ed25519:NEWKEY...", "url": "http://example.org" }
       ]
       ...
       ...
       ...
     ]
   }
   ```

   Example request payload:
   ```json
   REQUEST='{
     "prospective_epoch_id": 11,
     "proposal": {
       "participants": {
         "next_id": 13,
         "participants": <participants_with_new_entry>
       }
     }
   }'
   ```

3. **Create the vote command**
   ```bash
   near contract call-function as-transaction v1.signer vote_new_parameters      json-args "$REQUEST"      prepaid-gas '100.0 Tgas'      attached-deposit '0 NEAR'      sign-as $YOUR_MPC_NEAR_ACCOUNT      network-config mainnet      sign-with-keychain send
   ```


After all participants have voted, the contract will move to a resharing phase.  
You can see this in the node logs (TBD) [#906](https://github.com/near/mpc/issues/906)

And when the resharing has finished look for… (TBD) [#906](https://github.com/near/mpc/issues/906)

# MPC Node Upgrades

From time to time, MPC nodes will need to be upgraded.  
This section describes how to vote for a new MPC Docker image hash and how to securely upgrade the MPC node in the CVM. 

When a new MPC node is released, the release will along with precompiled binaries contain the following information:
* Git commit used to build the MPC image, identified by the release tag.
* Link to Docker Hub (or another repository) containing the released MPC Docker image.  
* Hash of the MPC Docker image (note: this is not the same as the Docker image manifest hash published on Docker Hub).  

> **Important:** Each operator is responsible for verifying that the MPC Docker image hash being voted for corresponds to the intended MPC Git commit, and for performing their own due diligence on the code.

**Main Steps:**

1. Verify that the Docker image hash matches the expected MPC node Git commit.  
2. Vote for the new Docker image hash in the contract.  
3. Upgrade the MPC node running in your CVM. 


##  MPC node Image/code inspection

The following steps allow you to inspect the code that was used to build the docker image: Assuming you want to vote for a docker image hash of Sha256:xyz… 
1.  Download the MPC code from \<TBD\>  [#907](https://github.com/near/mpc/issues/907)
2.  Compile it using the reproduce build script \<TBD\>  [#907](https://github.com/near/mpc/issues/907)
[#693](https://github.com/near/mpc/issues/693) 
[#635](https://github.com/near/mpc/issues/635) 

3.  Dockerize it using the script \<TBD\>   [#907](https://github.com/near/mpc/issues/907)
4.  Make sure the image hash you get is Sha256:xyz…  
5.  Do your own self do diligence on the code/binary   

##  **Voting for the Image** 

After deciding to vote for a new MPC Docker image hash, each participant submits a vote for that hash.  
A **threshold** number of participant votes is required in order for the vote to pass.



Vote Command using NEAR CLI:

```bash
  near contract call-function as-transaction \
  v1.signer-prod.testnet \
  vote_code_hash \
  json-args '{"code_hash": "<IMAGE_HASH>"}' \
  prepaid-gas '100.0 Tgas' \
  attached-deposit '0 NEAR' \
  sign-as <your-account-id> \
  network-config testnet \
  sign-with-keychain \
  send
```

The **IMAGE_HASH** argument must be provided as a JSON array of 32 numbers, where each number is a byte (0–255) of the SHA-256 digest.

For example, for the digest
````bash
4b08c2745a33aa28503e86e33547cc5a564abbb13ed73755937ded1429358c9d
````
the corresponding `IMAGE_HASH` is:

````bash
IMAGE_HASH="[75, 8, 194, 116, 90, 51, 170, 40, 80, 62, 134, 227, 53, 71, 204, 90,
 86, 74, 187, 177, 62, 215, 55, 85, 147, 125, 237, 20, 41, 53, 140, 157]"
 ````

TBD [#908](https://github.com/near/mpc/issues/908) Add here voting procedure.

## **Update the MPC node** 

After voting has finished, the MPC node will detect that there is a new approved MPC docker image hash register on the contract, and will download and save the hash into a secure location inside the CVM.

You can view this has happened by

TBD [#909](https://github.com/near/mpc/issues/909) \- add logs/screen shoot.

Following the hash update, you should upgrade the MPC node by following those steps:  
1\. Manually confirm the MPC dockerhub path/tag.  (optionly)   
2\. Update the dockerhub image path/tag in the user-config.conf  
3\. Restart the CVM.  

### Confirm that you have the correct dockerhub link to the approved MPC docker image.

* Assume the dockerhub link provided to you is  nearone/mpc-node-gcp:abc..  
* download the image to some local machine via the command:   
  “docker pull nearone/mpc-node-gcp:abc..  
* Retrieve the image hash via  
  * docker inspect nearone/mpc-node-gcp:abc | grep “Id”:  
* Check that you got "Id":"xyz…", that matches the hash you voted for.

# Updating the CVM `user-config.conf` with new registry information

If any of the following fields change, you must update your `user-config.conf`:

- `MPC_REGISTRY`  
- `MPC_IMAGE_NAME`  
- `MPC_IMAGE_TAGS`  

**Example:**
```ini
MPC_REGISTRY=registry.hub.docker.com
MPC_IMAGE_NAME=nearone/mpc-node-gcp
MPC_IMAGE_TAGS=SHA256:abc
```

---

## Steps

1. **Stop the CVM**  
2. **Update `user-config.conf`** with the new values  
3. **Start the CVM**

---

## Options for performing the update

### Manually (WebUI + file edit)

- You can only **start** and **stop** a CVM from the WebUI.  
- The **config must be edited manually**.  
- The config file can be found under the VM’s shared directory:

```
/meta-dstack/build/run/vm/<VM-ID>/shared
```

For example:

```bash
ubuntu@near-tdx-test02:/mnt/data/barak/test/meta-dstack/build/run/vm/e44abe87-6c5b-46ec-83b8-5d8357fd001a/shared$ ls -a
.  ..  .instance_info  .sys-config.json  .user-config  app-compose.json
```

Here, the file you need is `.user-config`.

---

### Via Command Line

- See the [VMM CLI user guide](https://github.com/Dstack-TEE/dstack/blob/master/docs/vmm-cli-user-guide.md).  
- The CLI script is located at:  
  `meta-dstack/dstack/vmm/src/vmm-cli.py`  

First, define environment variables (once per shell session):  

```bash

export VMM_URL=http://127.0.0.1:11100 # change to your port 
export VMM_CLI_PATH="meta-dstack/dstack/vmm/src/vmm-cli.py" # change to your meta-dstack location 
```

Then you can use `$VMM_CLI` for all commands:

```bash
# 1. Enumerate and find your VM ID
python $VMM_CLI_PATH --url $VMM_URL lsvm

# 2. Gracefully stop the VM
python $VMM_CLI_PATH --url $VMM_URL stop <vm-id>

# 3. Update user-config
python $VMM_CLI_PATH --url $VMM_URL update-user-config <vm-id> ./new-user-config.txt

# 4. Start the VM
python $VMM_CLI_PATH --url $VMM_URL start <vm-id>
```


### Restart the CVM 

If not done in the previous step, stop and start the CVM.  

The new MPC docker binary should be automatically pulled from docker hub, verified and launched, and a remote attestation will be sent to the contract.

You can see in the MPC node's logs (TBD) [#910](https://github.com/near/mpc/issues/910)that the image was updated, and that node has synced again. (TBD, [#910](https://github.com/near/mpc/issues/910) add logs).

# Trouble shooting 

TBD   [#912](https://github.com/near/mpc/issues/912)
Reviewers \- please add here more scenarios (with or without solutions) 

* do we have logs that indicate the node version/hash?  
* How to see what MPC node hash is expected by the launcher (docker-compose v.s file on disk)  
* Recovery \- how to erase the indexer state (e.g data folder)  
* …..

# Transition phase {#transition-phase}

During the transition phase, both MPC nodes with TEE and without TEE will be allowed.   
After the transition phase is completed, only MPC nodes with valid remote attestation of a valid TEE configuration will be allowed to join the MPC cluster, and any node without a valid TEE configuration will be kicked out of the cluster.

