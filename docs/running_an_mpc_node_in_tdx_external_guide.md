# MPC Node Deployment in TDX  \- External Guide

# Introduction

Chain signatures is a Multi-Party Computation (MPC) service that lets you sign arbitrary payloads by calling a smart contract and receiving a signature. The returned signature can be used to derive public keys on external chains (for example, Ethereum or Bitcoin).

This guide walks you through deploying a self-hosted MPC node on a bare-metal server using Intel Trusted Domain Extensions (TDX). The node runs inside a Confidential VM (CVM) for isolation and attestation.

We use Dstack (from Phala) to orchestrate the environment and run the MPC container inside the CVM

# Limitations and Restrictions

 **Important:**
You cannot migrate an existing MPC node out of its CVM without data loss (for example: key share, P2P key). In addition, replacing or changing TDX-related hardware or dependencies (e.g., a CPU swap) may render the data unrecoverable.

# Main difference between TEE and non TEE MPC nodes.

From an operator’s perspective, the key differences between a **TEE-based** MPC node and a **non-TEE** node are:

For a full architecture review of the TEE-based MPC, see: [design doc](securing_mpc_with_tee_design_doc.md)



| Category/feature | Non TEE | With TEE |
| :---- | :---- | :---- |
|  |  |  |
| Hardware Setup | Any hardware that meets the bandwidth,compute and memory requirements.  | Additional requirements to support Intel TDX Hardware.  |
| Deployment  | Deploy an MPC Docker image directly. | Use Dstack interface to deploy a Launcher docker image that will deploy the MPC node.  |
| Node Account key and P2P key | Generated by the operator, and provided to the node. | Generated by the node. Private keys are never exposed outside of  the CVM. Public keys are exported by  the node. And added to the operator's account. |
| Tooling, debug,...(TBD) [#892](https://github.com/near/mpc/issues/892)|  |  |

#  Prerequisites and Requirements

## Hardware Requirements 

* Have a TDX enabled, bare metal, server.  
    
  Note \- we currently only support bare metal and  do not support virtualized TDX solutions (such as GCP)  
  

* Intel Xeon 5th/6th Generation CPU (TDX Support) and 8 RAM slots filled  
  See [Intel TDX HW requirements](https://cc-enabling.trustedservices.intel.com/intel-tdx-enabling-guide/03/hardware_selection/)  
* Memory \- 64GB  
* (v)Cores \- 8  
* Disk space \- 500GB, SSD NVMe or similar performance

For a list of supported cloud providers offering bare metal servers with Intel TDX, see [Cloud Providers Supporting Bare Metal Servers with Intel TDX](./cloud-providers-tdx.md).


##  General 

* Firewall:allow ingress port 80 (MPC), 24567 (near) and port 8080 (web)  
* Assign a static public IP for access towards machine from outside

## Create DNS A record (optional). 

Although a node can be accessed using a public IP address, it is recommended to use a domain name instead. Using a domain name allows some flexibility in case of public IP address change/repurpose or failover scenarios. To use a domain name, one must register a DNS A record. Some recommended providers:

* [Namecheap](https://www.namecheap.com/support/knowledgebase/article.aspx/319/2237/how-can-i-set-up-an-a-address-record-for-my-domain/)  
* [Cloudflare](https://developers.cloudflare.com/dns/manage-dns-records/how-to/create-dns-records/)

## TDX and Dstack setup

This section describes how to enable TDX on your machine (BIOS, Operating System (OS) and software configurations), and also describes how to install and configure Dstack. 

Make sure to follow the 3 steps below in order to have a working TDX machine with Dstack setup.

1. A bare metal TDX server setup following [canonical/tdx](https://github.com/canonical/tdx/blob/main/README.md)  
   Follow steps 1-4 (basic TDX setup configuration)  and steps 9.1-9.2 (Setup Remote Attestation) in the canonical guide to make sure you have a working TDX setup.

2. Dstack setup following [https://github.com/Dstack-TEE/dstack](https://github.com/Dstack-TEE/dstack)  
   Follow the steps in [build-and-play-locally](https://github.com/Dstack-TEE/dstack?tab=readme-ov-file#build-and-play-locally) up until (but not including) “[deploy an App](https://github.com/Dstack-TEE/dstack?tab=readme-ov-file#deploy-an-app)”.  
   Note \- you won’t need to enable or run KMS and TProxy (gateway).  
   


[#693](https://github.com/near/mpc/issues/693) 
[#635](https://github.com/near/mpc/issues/635) 

TBD  [#894](https://github.com/near/mpc/issues/894)  : We need to choose a specific github commit and use the reproducible build script  in order to  generate the OS image. [https://github.com/Dstack-TEE/meta-dstack?tab=readme-ov-file\#reproducible-build-the-guest-image](https://github.com/Dstack-TEE/meta-dstack?tab=readme-ov-file#reproducible-build-the-guest-image)  
Or we can choose a specific dstack image like 0.52

3. Setup local gramine-sealing-key-provider (see details [below](#setting-up-local-gramine-sealing-key-provider:)).  
   

### Dstack Configuration
In this section we describe specific Dstack configuration changes needed for deploying the MPC node in Dstack.

In vmm.toml: 

- Update `cvm.port_mapping.range` to include port number **24567**  
- In the `[cvm]` section add: `max_disk_size = 1000`


### Setting up local gramine-sealing-key-provider: {#setting-up-local-gramine-sealing-key-provider:}

In our solution we are using the gramine-sealing-key-provider that is running in an SGX enclave in order to generate a key (based on the TDX measurements and SGX enclaves hardware sealing key) that is used to encrypt the CVM’s file system.  
**Note** - This key is tied to the platform, and losing it will cause the CVM to lose the ability to decrypt the drive on the following VM boot.

#### Setup instructions:

1\.  Follow the [canonical/tdx](https://github.com/canonical/tdx/blob/main/README.md) setup if not done before  especially step 9.1-2  (establishing a SGX PCCS - Provisioning Certification Caching Service ) 

2\. An instance of [**gramine-sealing-key-provider**](https://github.com/MoeMahhouk/gramine-sealing-key-provider) is required to be deployed on the host machine.   
The instance can be deployed by running the script [run.sh](https://github.com/Dstack-TEE/dstack/blob/master/key-provider-build/run.sh). For more information see  [local-key-provider-from-phala](https://github.com/Dstack-TEE/dstack/tree/master/kms#local-key-provider-mode-1)    

**Note:** Having Docker installed is a prerequisite for this step.

You can find mr_enclave value (of the sgx key provider)  by printing the container logs:  
docker logs gramine-sealing-key-provider 2\>&1 | grep mr_enclave | head -n 1

Make sure the  mr_enclave of the sgx key provider is 1b7a49378403249b6986a907844cab0921eca32dd47e657f3c10311ccaeccf8b

TBD [#895](https://github.com/near/mpc/issues/895) - regenerate the MR_enclave again check it is the same. 

# MPC Node Setup and Deployment

This section will describe how to configure and deploy your MPC node inside a CVM.

Including 

*  Creating a Near account for your node

*  Preparing  a configuration file based on [user-config.conf](https://github.com/near/mpc/blob/main/tee_deployment/user-config.conf)

* Creating a docker compose file for the launcher based on [launcher\_docker\_compose.yaml](https://github.com/near/mpc/blob/main/tee_deployment/launcher_docker_compose.yaml).  
* Configuring and starting your CVM with the MPC node.  
* Accessing mpc docker logs.  
* Retrieve keys from the CVM.  
* Add the node key to your Near account.

## Create a Near account for your node 

TBD [#896](https://github.com/near/mpc/issues/896) -Do we need a pre step for generating account private/public key pair?

### **Mainnet**

Using a mainnet named account is required. To create one you can use a known wallet like [https://www.mynearwallet.com](https://www.mynearwallet.com) or [https://wallet.meteorwallet.app](https://wallet.meteorwallet.app) or fund it yourself:

```
# Example of account creation using near cli
$ near account create-account fund-myself <ACCOUNT_NAME> '<AMMOUNT TO FUND> NEAR' autogenerate-new-keypair save-to-keychain sign-as <SIGNER_ACCOUNT_ID> network-config mainnet
```

### **Testnet**

If you're using testnet, the easiest way to get started is to create an account sponsored by faucet \- the [Near command line interface](https://github.com/near/near-cli-rs) can set this up for you. Public and private keys were generated here. You can also use an existing account if you have one.

```
# Example of account creation using near cli
$ near account create-account sponsor-by-faucet-service <ACCOUNT_NAME> use-manually-provided-public-key <PUBLIC_KEY> network-config testnet create
For details please refer to near account documentation.
```

##   Prepare MPC container environment variables

Create a user-config.conf file based on the [user-config.conf](https://github.com/near/mpc/blob/main/tee_deployment/user-config.conf) . 

```
# MPC docker image local override
MPC_IMAGE_NAME=nearone/mpc-node-gcp
MPC_IMAGE_TAGS=latest
MPC_REGISTRY=registry.hub.docker.com

# MPC node settings
MPC_ACCOUNT_ID=$MY_MPC_NEAR_ACCOUNT_ID
MPC_LOCAL_ADDRESS=127.0.0.1
MPC_SECRET_STORE_KEY=AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
MPC_CONTRACT_ID=$CONTRACT_ID # v1.signer-prod.testnet for Testnet or v1.signer for Mainnet
MPC_ENV=$CHAIN_ID # testnet or mainnet
MPC_HOME_DIR=/data
RUST_BACKTRACE=full
RUST_LOG=mpc=debug,info

NEAR_BOOT_NODES=$BOOT_NODES


# Port forwarding 
PORTS=8080:8080,24567:24567,80:80

```

Adjust the variables as per your environment.

* MPC_ACCOUNT_ID `-` use the near account ID that was created in the previous step  
  MPC\_CONTRACT\_ID is **v1.signer-prod.testnet** for testnet and **v1.signer** for mainnet 
* PORTS: Those are the port forwarding rules for the MPC container. Those should be a subset of the port forwarding for the CVM that are defined [Port Mapping](https://github.com/near/mpc/blob/main/docs/running_an_mpc_node_in_tdx_external_guide.md#using-the-web-interface)
*  A fresh set of boot nodes can be selected using Testnet/Mainnet RPC endpoints. Copy at least 4-5 nodes from curl results into NEAR\_NOOT\_NODES variable:

```
curl -X POST https://rpc.[testnet|mainnet].near.org \
  -H "Content-Type: application/json" \
  -d '{
        "jsonrpc": "2.0",
        "method": "network_info",
        "params": [],
        "id": "dontcare"
      }'| \
jq -r '.result.active_peers[]  as $active_peer  | "\($active_peer.id)@\($active_peer.addr)"' |\
paste -sd',' -
```


## Preparing a docker compose

Take the docker compose file from [https://github.com/near/mpc/blob/main/tee\_deployment/launcher\_docker\_compose.yaml](https://github.com/near/mpc/blob/main/tee_deployment/launcher_docker_compose.yaml)

Replace MPC docker image digest hash (DEFAULT\_IMAGE\_DIGEST field) with the hash of the latest hash that is voted on the contract or the latest hash published by NEAR.

DEFAULT\_IMAGE\_DIGEST=sha256:4b08c2745a33aa28503e86e33547cc5a564abbb13ed73755937ded1429358c9d 

TBD  [#898](https://github.com/near/mpc/issues/898) \- add how to get it from the contract?  
TBD [#899](https://github.com/near/mpc/issues/899)  \- where should it be published?  
   
Note \-  the [launcher\_docker\_compose.yaml](https://github.com/near/mpc/blob/main/tee_deployment/launcher_docker_compose.yaml) is measured, and the measurements are part of the remote attestation. Make sure not to change any other fields or values (including any white spaces).


## Required Ports and Port Collisions 

MPC nodes use a fixed set of ports for communication and telemetry.  
This creates a limitation when trying to run both **mainnet** and **testnet** nodes on the same physical server, since both sets of nodes attempt to bind to the same ports.

---

- **Single network per machine**: By default, running both mainnet and testnet on the same machine is not supported because of port collisions.  
- **Workaround with multiple IPs**: It is possible to run multiple nodes (e.g., one mainnet and one testnet) on the same host if the server is configured with **multiple external IP addresses**.  
  - Each node binds to the required ports (see below) on a separate IP.  
  - Additional IP/port routing on the local machine may be required.  

---

### Required Ports

| Port   | Purpose                                                                 |
|--------|-------------------------------------------------------------------------|
| **80** | Node-to-node communication (port override convention)                   |
| **24567** | Decentralized state sync                                               |
| **8080** | Debug and telemetry collection, plus the new `getdata` endpoint         |
| **3030** | Debug and telemetry collection                                         |


## Configuring and starting the MPC binary in a CVM

There are 2 ways to manage the VM that will run the MPC node.

1\. Using the Web interface  
2\. Using a script.

Note \- both methods provide the same functionality. The Web interface provides a more manual approach and control. While the script is useful for automating processes.  

### **Using the Web interface** 

Follow the [Dstack guide](https://github.com/Dstack-TEE/dstack?tab=readme-ov-file#deploy-an-app) (deploy an App): 

Use the following custom settings for MPC:

1. Launcher docker compose file \- provided above.  
2. VM HW setting: (use exactly those settings, since vCPU/Memory are measured )  
    vCPU number=8  ,   Memory \= 64GB, disk \= 500 GB  
3. Pre script  \- empty.  
4. user-config \- provided above  
5. KMS=disable, Local Keyprovier=enabled, Tproxy=disable, public logs=enabled,public sysinfo=enabled,pin NUMA=disabled  
6. Port mapping: (taken from the list above)  
   Public 80:80  (main node to node communication port)   
   Public 24567:24567 (required for decentralized state sync)  
   Public 8080:8080  (required for collecting debug and telemetry information)   
   Local 3030:3030:   (use public with you want the debug metrics to be available on the internet)  
   Local <dstack_agent_port>:8090: (required for access CVM information and container logs)   
     
7.  Key Provider ID: (The MrEnclave for the sgx local key provider) 1b7a49378403249b6986a907844cab0921eca32dd47e657f3c10311ccaeccf8b

 

![CVM Web Page](./attachments/VMM_web_page.png)


###  Using the script: 

Use the script [deploy-launcher.sh](https://github.com/near/mpc/blob/main/tee_deployment/deploy-launcher.sh) described here   
[https://github.com/near/mpc/blob/main/tee\_deployment/deploy\_launcher\_guide.md](https://github.com/near/mpc/blob/main/tee_deployment/deploy_launcher_guide.md)  
To configure and start your VM.

## Accessing MPC (or Launcher) Docker Logs

### Overview
Dstack provides a dedicated web page to view CVM information, including links to the Docker logs.  
More details can be found in [Phala's guide](https://github.com/Dstack-TEE/dstack?tab=readme-ov-file#deploy-an-app).  

---

### Local Access
The web page is available on the **TDX server**  at **`dstack_agent_port`** configured earlier in [Using the Web Interface](#using-the-web-interface).  

Open in your browser:  

```
http://localhost:<dstack_agent_port>
```

---

### Remote Access
If you need to access the web page from another machine, set up SSH port forwarding.  

For example, if `dstack_agent_port = 8090`:  

```bash
ssh -NL 17190:localhost:8090 USER_NAME@TDX_SERVER
```

Then open:  

```
http://localhost:17190
```

---

### Example / Screenshot
![CVM Web Page](./attachments/CVM_web_page.png)


## Retrieve public keys from the MPC node. 

There are 2 keys that should be retrieved from node.  

* P2P  (near\_p2p\_public\_key)- this key is used by the nodes to authenticate with one another. This key needs to be registered on the contract. (see details below)  
* Node Account Key (near\_signer\_public\_key) \- this key is used by the node to issue operations such as “vote\_reshared”.  
  This key needs to be added to the Near account that was created in the step above.

### Retrieve the node account key and P2P key.

In order to retrieve the node account key and the P2P key. On your server   
Call the HTTP end point [http://localhost:8080/public\_data](http://localhost:8080/public_data)  \- and search for near\_signer\_public\_key and near\_p2p\_public\_key

```
{"near_signer_public_key":"ed25519:B2JvaYmgzfXsvCxrqd4nBrBt8jo9ReqUZatG3dAZEBv5","near_p2p_public_key":"ed25519:5SiS1SJiABiM79Yt6uEjMabAT9UguQY9hSyF7xfHLGYt"
```

Sample curl command to extract the keys:

```
$curl -s http://<IP>:8080/public_data | jq -r '.near_signer_public_key'
$ed25519:B2JvaYmgzfXsvCxrqd4nBrBt8jo9ReqUZatG3dAZEBv5$curl -s http://<IP>:8080/public_data | jq -r '.near_p2p_public_key'$ed25519:5SiS1SJiABiM79Yt6uEjMabAT9UguQY9hSyF7xfHLGYt

```

### **Add Node Account Key to your account:**

TBD  [#902](https://github.com/near/mpc/issues/902) \- add commands here

## 

# Joining the MPC Cluster

After the MPC node has been deployed, the next steps are:

* Register the node’s attestation information on the contract,   
* Wait for the node to fully sync  
* Vote for joining the node to the MPC cluster.  
  Note \- this step needs to be done by all the operators. 

  To join the cluster there are 1-2 contract commands that need to be called by the operator.  
  vote\_new\_parameters  and (possibly) submit\_participant\_info.

##  Submitting participate info (Submit\_participant\_info)

This method needs to be called in order to register the node attestation information on the contract and prove that the node is running inside a CVM with a valid configuration.

Node \- during the [transition phase](#transition-phase) \- this step is optional in case you want to register a node without TEE.

After the MPC node is fully synced, the node will check if the operator has added the node’s account keys to the operator's near contract. If so, the MPC node will send to the contract its attestation information via the submit\_participant\_info contract method.

This command can’t be called unless the key was added to the account, otherwise the call will fail due to lack of funds associated with the key.

**Note \- Calling this method will cost TBD [#903](https://github.com/near/mpc/issues/903) Near to none participants.**  
**So you need to have this amount associated with your account.**

In case the node failed to call submit\_participant\_info. The operator can do it on its behalf, by retrieving the attestation info from \<IP\>:8080/public\_info and submitting it to the contract.  
TBD [#904](https://github.com/near/mpc/issues/904) \- need to add some more details or script on how to do it.

## Wait for node indexer to sync

Wait till near-indexer state sync is completed. It will take a few hours. Check the docker container logs for sync progress. Indexer sync status is also available via metrics page:

```
curl http://127.0.0.1:3030/metrics | grep near_sync_status
# Sync is completed when status is 0: 
near_sync_status 0

```

## Voting \[ vote\_new\_parameters\]

Ask other members to vote for your candidacy via the vote\_new\_parameters contract method. 

```
TBD [#905](https://github.com/near/mpc/issues/905)  near CLI command line 
```

After all participants have voted, the contract will move to a resharing phase.  
You can see this in the node logs (TBD) [#906](https://github.com/near/mpc/issues/906)

And when the resharing has finished look for… (TBD) [#906](https://github.com/near/mpc/issues/906)

# MPC Node Upgrades:

From time to time, there will be a need to upgrade the MPC node.  
This section describes how to vote for a new MPC docker image hash, and how to securely upgrade the MPC node in the CVM. 

It is each operator’s responsibility to verify that the MPC docker image hash he votes for, corresponds to a specific MPC git commit, and do his own due diligence on the code. 

Main steps are:

1. Verify that a specific docker image hash corresponds to a specific MPC node git commit.   
2. Vote for the new node hash in the contract  
3. Upgrade the MPC node running in your CVM.

##  MPC node Image/code inspection

The following steps allow you to inspect the code that was used to build the docker image: Assuming you want to vote for a docker image hash of Sha256:xyz… 
1.  Download the MPC code from \<TBD\>  [#907](https://github.com/near/mpc/issues/907)
2.  Compile it using the reproduce build script \<TBD\>  [#907](https://github.com/near/mpc/issues/907)
[#693](https://github.com/near/mpc/issues/693) 
[#635](https://github.com/near/mpc/issues/635) 

3.  Dockerize it using the script \<TBD\>   [#907](https://github.com/near/mpc/issues/907)
4.  Make sure the image hash you get is Sha256:xyz…  
5.  Do your own self do diligence on the code/binary   

##  **Voting for the image** 

TBD [#908](https://github.com/near/mpc/issues/908) Add here voting procedure.

## **Update the MPC node** 

After voting has finished, the MPC node will detect that there is a new approved MPC docker image hash register on the contract, and will download and save the hash into a secure location inside the CVM.

You can view this has happened by

TBD [#909](https://github.com/near/mpc/issues/909) \- add logs/screen shoot.

Following the hash update, you should upgrade the MPC node by following those steps:  
1\. Manually confirm the MPC dockerhub path/tag.  (optionly)   
2\. Update the dockerhub image path/tag in the user-config.conf  
3\. Restart the CVM.  

### Confirm that you have the correct dockerhub link to the approved MPC docker image.

* Assume the dockerhub link provided to you is  nearone/mpc-node-gcp:abc..  
* download the image to some local machine via the command:   
  “docker pull nearone/mpc-node-gcp:abc..  
* Retrieve the image hash via  
  * docker inspect nearone/mpc-node-gcp:abc | grep “Id”:  
* Check that you got "Id":"xyz…", that matches the hash you voted for.

# Updating the CVM `user-config.conf` with new registry information

If any of the following fields change, you must update your `user-config.conf`:

- `MPC_REGISTRY`  
- `MPC_IMAGE_NAME`  
- `MPC_IMAGE_TAGS`  

**Example:**
```ini
MPC_REGISTRY=registry.hub.docker.com
MPC_IMAGE_NAME=nearone/mpc-node-gcp
MPC_IMAGE_TAGS=SHA256:abc
```

---

## Steps

1. **Stop the CVM**  
2. **Update `user-config.conf`** with the new values  
3. **Start the CVM**

---

## Options for performing the update

### Manually (WebUI + file edit)

- You can only **start** and **stop** a CVM from the WebUI.  
- The **config must be edited manually**.  
- The config file can be found under the VM’s shared directory:

```
/meta-dstack/build/run/vm/<VM-ID>/shared
```

For example:

```bash
ubuntu@near-tdx-test02:/mnt/data/barak/test/meta-dstack/build/run/vm/e44abe87-6c5b-46ec-83b8-5d8357fd001a/shared$ ls -a
.  ..  .instance_info  .sys-config.json  .user-config  app-compose.json
```

Here, the file you need is `.user-config`.

---

### Via Command Line

- See the [VMM CLI user guide](https://github.com/Dstack-TEE/dstack/blob/master/docs/vmm-cli-user-guide.md).  
- The CLI script is located at:  
  `meta-dstack/dstack/vmm/src/vmm-cli.py`  

First, define environment variables (once per shell session):  

```bash

export VMM_URL=http://127.0.0.1:11100 # change to your port 
export VMM_CLI_PATH="meta-dstack/dstack/vmm/src/vmm-cli.py" # change to your meta-dstack location 
```

Then you can use `$VMM_CLI` for all commands:

```bash
# 1. Enumerate and find your VM ID
python $VMM_CLI_PATH --url $VMM_URL lsvm

# 2. Gracefully stop the VM
python $VMM_CLI_PATH --url $VMM_URL stop <vm-id>

# 3. Update user-config
python $VMM_CLI_PATH --url $VMM_URL update-user-config <vm-id> ./new-user-config.txt

# 4. Start the VM
python $VMM_CLI_PATH --url $VMM_URL start <vm-id>
```


### Restart the CVM 

If not done in the previous step, stop and start the CVM.  

The new MPC docker binary should be automatically pulled from docker hub, verified and launched, and a remote attestation will be sent to the contract.

You can see in the MPC node's logs (TBD) [#910](https://github.com/near/mpc/issues/910)that the image was updated, and that node has synced again. (TBD, [#910](https://github.com/near/mpc/issues/910) add logs).

# Trouble shooting 

TBD   [#912](https://github.com/near/mpc/issues/912)
Reviewers \- please add here more scenarios (with or without solutions) 

* do we have logs that indicate the node version/hash?  
* How to see what MPC node hash is expected by the launcher (docker-compose v.s file on disk)  
* Recovery \- how to erase the indexer state (e.g data folder)  
* …..

# Transition phase {#transition-phase}

During the transition phase, both MPC nodes with TEE and without TEE will be allowed.   
After the transition phase is completed, only MPC nodes with valid remote attestation of a valid TEE configuration will be allowed to join the MPC cluster, and any node without a valid TEE configuration will be kicked out of the cluster.

